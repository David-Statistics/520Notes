# Chapter 3 - Common Families of Distributions {#ch3}

## 9/21/2016


```{block2, note-text, type = 'rmddefinition'}
A *single distribution* is completely specified (e.g. Gamma(3,2)).  
A *family of distributions* is defined by a functional form and parameter space containing more than 1 element (e.g. Uniform($a$,$b$); $\Theta = \{(a,b) : a,b, \in \mathbb{R}\})$

```

### Discrete Uniform $(N)$

$$P(X = x) = \frac{1}{N}; x \in \{1,2,3,...\}$$

Typical example: Die  
\[
\begin{aligned}
  E[X] &= \sum_{i=1}^N i\frac{1}{N} \\
    &= \frac{1}{N}\frac{N(N+1)}{2} \\
    &=\frac{N+1}{2} \\
  E[X^2] &= \sum_{i=1}^N i^2\frac{1}{N} \\
    &= \frac{1}{N}\frac{N(N+1)(2N + 1)}{6} \\
    &= \frac{(N+1)(2N+1)}{6} \\
  V[X] &= E[X^2] - E[X]^2 \\
    &= \frac{(N+1)(2N+1)}{6} - \frac{(N+1)^2}{4} \\
    &= \frac{2(N+1)(2N+1) - 3(N+1)^2}{12} \\
    &= \frac{(N+1)((4N+2) - (3N+3))}{12} \\
    &= \frac{(N+1)(N-1)}{12}
\end{aligned}
\] 

### Hypergeometric $(R,W,n)$

Suppose an urn has $R$ red balls and $W$ white balls and suppose $n$ balls are sampled without replacement. Let $T_n$ be the number of red balls sampled.
\[
\begin{aligned}
  T_n &= \mbox{Hypergeometric}(R,W,n) \\
  P(T_n = k) &= \frac{{R \choose k}{W \choose n-k}}{{R + W \choose n}}; k \in \{ \max(0, n-W), ..., \min(R,n)\}
\end{aligned}
\]
Note that if we see $T_n = X_1 + ... + X_n$, the $X_i$ are **not** independent.
\[
\begin{aligned}
  E[T_n] &= E[\sum X_i] = \frac{nR}{R+W} \\
  V[T_n] &= np(1-p)\left(1 - \frac{n-1}{R+W-1}\right)
\end{aligned}
\]
In the variance, $np(1-p)$ is the binomial variance while $\left(1 - \frac{n-1}{R+W-1}\right)$ is the *finite sample correction*.

```{block2, type = 'rmdexample'}
Last Christmas, the kitchen had 10 white lights and the living room had 20 colored lights 5 of the 30 failed. What is the probability exactly 3 were colored?

Urn1:  

$$P(T_5 = 3) = \frac{{20 \choose 3} {10 \choose 2}}{{30 \choose 5}} $$

This is a Hypergeometric(20,10,5) distribution and we're sampling the failed lights.

Urn2:  

$$ P(T_{20} = 3) = \frac{{5 \choose 3} {25 \choose 17}}{{30 \choose 20}}$$

This is a Hypergeometric(5,25,20) distribution and we're sampling the white lights.

```

## 9/26/2016

Let $X_1, ... \stackrel{iid}{\sim}$ Bernoulli($p$).  
Fix $n \in \mathbb{N}$,
$$ T_n = X_1 + X_2 + ... + X_n \sim \mbox{Binomial}(n,p) $$
Fix $r \in \mathbb{N}$,  
$W_r = $ number of trials til $r$ successes
$$ W_r \sim \mbox{NegativeBinomial}(r,p) $$ 
Fix $i \in \mathbb{N}$,
$$ Y_i = W_i - W_{i-1} \sim \mbox{Geometric}(p) $$

### Binomial$(n,p)$

$$ P(T_n = k) = {n \choose k} p^k (1-p)^{n-k} $$
Note that since the $X_i$ are independent, calculate the expected value and variance using the sum of the $X_i$.
\[
\begin{aligned}
  E[T_n] &= np \\
  V[T_n] &= np(1-p)
\end{aligned}
\]

```{block2, type = 'rmdtheorem'}
If $T_m \sim$ Binom($m,p$) and $T_n \sim$ Binom($n,p$) and $T_m \perp T_n$ then,

$$ T_m + T_n \sim \mbox{Binom}(m+n,p)$$

```

### Geometric($p$)

Consider a typical interarrival time, $Y = Y_1$.  
The event that $[Y = k]$ occurs if and only if the first success follows $k-1$ failures.
$$ P(Y = k) = (1-p)^{k-1}p; \qquad k = 0,1,... $$

```{block2, type = 'rmdtheorem'}
**Tail Probability**

\[
\begin{aligned}
  P(Y > k) &= \sum_{i = k+1}^\infty (1-p)^{k-1}p \qquad j = i - (k+1) \\
    &= \sum_{j=0}^\infty (1-p)^{j+k}p \\
    &= (1-p)^k\sum_{j=0}^\infty (1-p)^jp \\
    &= (1-p)^k \frac{p}{1-(1-p)} \\
    &= (1-p)^k; \qquad k = 0,1,...
\end{aligned}
\]

Thus, we have 

$$ F_Y(y) = 1 - (1-p)^y $$

```

```{block2, type = 'rmdtheorem'}
$Y_1, Y_2,...$ are iid Geometric($p$) random variables.

```

```{block2, type = 'rmdtheorem'}
If $Y \sim$ Geometric($p$) then $Y$ has the *lack of memory property*.

$$ P(Y > k +i | Y > k) = P(Y = i) $$
for $i \geq 1$ and fixed $k \geq 1$. 

**proof**:

Fix $k \in \mathbb{N}$ and $i \in \mathbb{N}$, then we have

\[
\begin{aligned}
  P(Y > k +i | Y > k) &= \frac{P(Y > k + i \cap Y > k)}{P(Y > k)} \\
    &= \frac{P(Y > k+i)}{P(Y > k)} \\
    &= \frac{(1-p)^{k+i}}{(1-p)^{k}} \\
    &= (1-p)^i \\
    &= P(Y > i)
\end{aligned}
\]

```

For the geometric distribution, we have,
\[
\begin{aligned}
  E[Y] &= \frac{1}{p} \\
  V[Y] &= \frac{1-p}{p^2} 
\end{aligned}
\]

**SHOULD ADD PROOFS HERE**

### Negative Binomial (NegBi($r,p$))

Let $W_r$ be the number of Bernoulli$(p)$ trials until $r$ successes. Note that $[W_r = k] = [\sum_{i=1}^n Y_i = k] = [T_{k-1} = r-1 \cap X_k = 1]$ all describe the same event.
$$ P(W_r = k) = {k-1 \choose r-1}p^r(1-p)^{k-r}; \qquad k = r, r+1, ... $$

```{block2, type = 'rmdtheorem'}
**Event Identity**

The event $[W_r > n]$ is equivalent to the event $[T_n < r]$ and thus any calculation involving $W_r$ can be replaced by a calculation involving $T_n$

```

Using the fact the relationship between the negative binomial and the geometric distribution, we have

\[
\begin{aligned}
  E[W_r] &= \frac{r}{p} \\
  V[W_r] &= \frac{r(1-p)}{p^2}
\end{aligned}
\]

```{block2, type = 'rmdtheorem'}
Suppose $W_r \sim$ NegBi($r,p$) and $W_s \sim$ NegBi($s,p$) and $W_r \perp W_s$. Then we have,

$$ W_r + W_s \sim \mbox{NegBi}(r+s,p) $$

```

## 9/28/16

```{block2, type = 'rmdtheorem'}
**Conditional Randomness**  
Let $X_1, ..., X_n$ denote a sequence of 0's and 1's. 

\[
\begin{aligned}
  P(X_1 = x_1, ..., X_n = x_n | X_1 + ... + X_n = T_n = m) &= \frac{\prod_{i = 1}^n p^{x_i}(1-p)^{1-x_i}}{{n \choose m} p^m(1-p)^{n-m}} \\
    &= \frac{p^{\sum_{i=1}^n x_i}(1-p)^{n - \sum_{i=1}^n x_i}}{{n \choose m} p^m(1-p)^{n-m}} \\
    &= \frac{p^{m}(1-p)^{n - m}}{{n \choose m} p^m(1-p)^{n-m}} \\
    &= \frac{1}{{n \choose m}}
\end{aligned}
\]

Thus, given that the sum of $n$ bernoullis is $m$ then, each permutation of success and failures is equally likely.

```

```{block2, type = 'rmdtheorem'}
Let $k < n$, now given that $T_n = m$, we have

\[
\begin{aligned}
  P(T_k = i | T_n = m) &= \frac{P(T_k = i) P(T_{(k+1):n} = m-i)}{P(T_n = m)} \\
    &= \frac{{k \choose i}p^i(1-p)^{k-i}{n-k \choose m-i}p^{m-i}(1-p)^{n-k-m+i}}{{n \choose m}p^m(1-p)^{n-m}} \\
    &= \frac{{k \choose i}{n-k \choose m-i}p^{m}(1-p)^{n-m}}{{n \choose m}p^m(1-p)^{n-m}} \\
    &= \frac{{k \choose i}{n-k \choose m-i}}{{n \choose m}}
\end{aligned}
\]

```


