[
["index.html", "STAT520 Fall 2016 Notes 1 Intro", " STAT520 Fall 2016 Notes David Clancy August - December 2016 1 Intro This is a compilation of notes from Colorado State’s STAT520 Class from the fall of 2016. This course is taught by Dr. Bailey Fosdick. "],
["ch1.html", "2 Chap 1", " 2 Chap 1 You can label chapter and section titles using {#label} after them, e.g., we can reference Chapter 4. If you do not manually label them, there will be automatic labels anyway, e.g., Chapter. Figures and tables with captions will be placed in figure and table environments, respectively. par(mar = c(4, 4, .1, .1)) plot(pressure, type = &#39;b&#39;, pch = 19) Figure 2.1: Here is a nice figure! Reference a figure by its code chunk label with the fig: prefix, e.g., see Figure 2.1. Similarly, you can reference tables generated from knitr::kable(), e.g., see Table 2.1. knitr::kable( head(iris, 20), caption = &#39;Here is a nice table!&#39;, booktabs = TRUE ) Table 2.1: Here is a nice table! Sepal.Length Sepal.Width Petal.Length Petal.Width Species 5.1 3.5 1.4 0.2 setosa 4.9 3.0 1.4 0.2 setosa 4.7 3.2 1.3 0.2 setosa 4.6 3.1 1.5 0.2 setosa 5.0 3.6 1.4 0.2 setosa 5.4 3.9 1.7 0.4 setosa 4.6 3.4 1.4 0.3 setosa 5.0 3.4 1.5 0.2 setosa 4.4 2.9 1.4 0.2 setosa 4.9 3.1 1.5 0.1 setosa 5.4 3.7 1.5 0.2 setosa 4.8 3.4 1.6 0.2 setosa 4.8 3.0 1.4 0.1 setosa 4.3 3.0 1.1 0.1 setosa 5.8 4.0 1.2 0.2 setosa 5.7 4.4 1.5 0.4 setosa 5.4 3.9 1.3 0.4 setosa 5.1 3.5 1.4 0.3 setosa 5.7 3.8 1.7 0.3 setosa 5.1 3.8 1.5 0.3 setosa You can write citations, too. For example, we are using the bookdown package (Xie 2016) in this sample book, which was built on top of R Markdown and knitr (Xie 2015). References "],
["literature.html", "3 Literature", " 3 Literature Here is a review of existing methods. "],
["ch3.html", "4 Chapter 3 - Common Families of Distributions 4.1 9/21/2016 4.2 9/26/2016 4.3 9/28/16", " 4 Chapter 3 - Common Families of Distributions 4.1 9/21/2016 A single distribution is completely specified (e.g. Gamma(3,2)). A family of distributions is defined by a functional form and parameter space containing more than 1 element (e.g. Uniform(\\(a\\),\\(b\\)); \\(\\Theta = \\{(a,b) : a,b, \\in \\mathbb{R}\\})\\) 4.1.1 Discrete Uniform \\((N)\\) \\[P(X = x) = \\frac{1}{N}; x \\in \\{1,2,3,...\\}\\] Typical example: Die \\[ \\begin{aligned} E[X] &amp;= \\sum_{i=1}^N i\\frac{1}{N} \\\\ &amp;= \\frac{1}{N}\\frac{N(N+1)}{2} \\\\ &amp;=\\frac{N+1}{2} \\\\ E[X^2] &amp;= \\sum_{i=1}^N i^2\\frac{1}{N} \\\\ &amp;= \\frac{1}{N}\\frac{N(N+1)(2N + 1)}{6} \\\\ &amp;= \\frac{(N+1)(2N+1)}{6} \\\\ V[X] &amp;= E[X^2] - E[X]^2 \\\\ &amp;= \\frac{(N+1)(2N+1)}{6} - \\frac{(N+1)^2}{4} \\\\ &amp;= \\frac{2(N+1)(2N+1) - 3(N+1)^2}{12} \\\\ &amp;= \\frac{(N+1)((4N+2) - (3N+3))}{12} \\\\ &amp;= \\frac{(N+1)(N-1)}{12} \\end{aligned} \\] 4.1.2 Hypergeometric \\((R,W,n)\\) Suppose an urn has \\(R\\) red balls and \\(W\\) white balls and suppose \\(n\\) balls are sampled without replacement. Let \\(T_n\\) be the number of red balls sampled. \\[ \\begin{aligned} T_n &amp;= \\mbox{Hypergeometric}(R,W,n) \\\\ P(T_n = k) &amp;= \\frac{{R \\choose k}{W \\choose n-k}}{{R + W \\choose n}}; k \\in \\{ \\max(0, n-W), ..., \\min(R,n)\\} \\end{aligned} \\] Note that if we see \\(T_n = X_1 + ... + X_n\\), the \\(X_i\\) are not independent. \\[ \\begin{aligned} E[T_n] &amp;= E[\\sum X_i] = \\frac{nR}{R+W} \\\\ V[T_n] &amp;= np(1-p)\\left(1 - \\frac{n-1}{R+W-1}\\right) \\end{aligned} \\] In the variance, \\(np(1-p)\\) is the binomial variance while \\(\\left(1 - \\frac{n-1}{R+W-1}\\right)\\) is the finite sample correction. Last Christmas, the kitchen had 10 white lights and the living room had 20 colored lights 5 of the 30 failed. What is the probability exactly 3 were colored? Urn1: \\[P(T_5 = 3) = \\frac{{20 \\choose 3} {10 \\choose 2}}{{30 \\choose 5}} \\] This is a Hypergeometric(20,10,5) distribution and we’re sampling the failed lights. Urn2: \\[ P(T_{20} = 3) = \\frac{{5 \\choose 3} {25 \\choose 17}}{{30 \\choose 20}}\\] This is a Hypergeometric(5,25,20) distribution and we’re sampling the white lights. 4.2 9/26/2016 Let \\(X_1, ... \\stackrel{iid}{\\sim}\\) Bernoulli(\\(p\\)). Fix \\(n \\in \\mathbb{N}\\), \\[ T_n = X_1 + X_2 + ... + X_n \\sim \\mbox{Binomial}(n,p) \\] Fix \\(r \\in \\mathbb{N}\\), $W_r = $ number of trials til \\(r\\) successes \\[ W_r \\sim \\mbox{NegativeBinomial}(r,p) \\] Fix \\(i \\in \\mathbb{N}\\), \\[ Y_i = W_i - W_{i-1} \\sim \\mbox{Geometric}(p) \\] 4.2.1 Binomial\\((n,p)\\) \\[ P(T_n = k) = {n \\choose k} p^k (1-p)^{n-k} \\] Note that since the \\(X_i\\) are independent, calculate the expected value and variance using the sum of the \\(X_i\\). \\[ \\begin{aligned} E[T_n] &amp;= np \\\\ V[T_n] &amp;= np(1-p) \\end{aligned} \\] If \\(T_m \\sim\\) Binom(\\(m,p\\)) and \\(T_n \\sim\\) Binom(\\(n,p\\)) and \\(T_m \\perp T_n\\) then, \\[ T_m + T_n \\sim \\mbox{Binom}(m+n,p)\\] 4.2.2 Geometric(\\(p\\)) Consider a typical interarrival time, \\(Y = Y_1\\). The event that \\([Y = k]\\) occurs if and only if the first success follows \\(k-1\\) failures. \\[ P(Y = k) = (1-p)^{k-1}p; \\qquad k = 0,1,... \\] Tail Probability \\[ \\begin{aligned} P(Y &gt; k) &amp;= \\sum_{i = k+1}^\\infty (1-p)^{k-1}p \\qquad j = i - (k+1) \\\\ &amp;= \\sum_{j=0}^\\infty (1-p)^{j+k}p \\\\ &amp;= (1-p)^k\\sum_{j=0}^\\infty (1-p)^jp \\\\ &amp;= (1-p)^k \\frac{p}{1-(1-p)} \\\\ &amp;= (1-p)^k; \\qquad k = 0,1,... \\end{aligned} \\] Thus, we have \\[ F_Y(y) = 1 - (1-p)^y \\] \\(Y_1, Y_2,...\\) are iid Geometric(\\(p\\)) random variables. If \\(Y \\sim\\) Geometric(\\(p\\)) then \\(Y\\) has the lack of memory property. \\[ P(Y &gt; k +i | Y &gt; k) = P(Y = i) \\] for \\(i \\geq 1\\) and fixed \\(k \\geq 1\\). proof: Fix \\(k \\in \\mathbb{N}\\) and \\(i \\in \\mathbb{N}\\), then we have \\[ \\begin{aligned} P(Y &gt; k +i | Y &gt; k) &amp;= \\frac{P(Y &gt; k + i \\cap Y &gt; k)}{P(Y &gt; k)} \\\\ &amp;= \\frac{P(Y &gt; k+i)}{P(Y &gt; k)} \\\\ &amp;= \\frac{(1-p)^{k+i}}{(1-p)^{k}} \\\\ &amp;= (1-p)^i \\\\ &amp;= P(Y &gt; i) \\end{aligned} \\] For the geometric distribution, we have, \\[ \\begin{aligned} E[Y] &amp;= \\frac{1}{p} \\\\ V[Y] &amp;= \\frac{1-p}{p^2} \\end{aligned} \\] SHOULD ADD PROOFS HERE 4.2.3 Negative Binomial (NegBi(\\(r,p\\))) Let \\(W_r\\) be the number of Bernoulli\\((p)\\) trials until \\(r\\) successes. Note that \\([W_r = k] = [\\sum_{i=1}^n Y_i = k] = [T_{k-1} = r-1 \\cap X_k = 1]\\) all describe the same event. \\[ P(W_r = k) = {k-1 \\choose r-1}p^r(1-p)^{k-r}; \\qquad k = r, r+1, ... \\] Event Identity The event \\([W_r &gt; n]\\) is equivalent to the event \\([T_n &lt; r]\\) and thus any calculation involving \\(W_r\\) can be replaced by a calculation involving \\(T_n\\) Using the fact the relationship between the negative binomial and the geometric distribution, we have \\[ \\begin{aligned} E[W_r] &amp;= \\frac{r}{p} \\\\ V[W_r] &amp;= \\frac{r(1-p)}{p^2} \\end{aligned} \\] Suppose \\(W_r \\sim\\) NegBi(\\(r,p\\)) and \\(W_s \\sim\\) NegBi(\\(s,p\\)) and \\(W_r \\perp W_s\\). Then we have, \\[ W_r + W_s \\sim \\mbox{NegBi}(r+s,p) \\] 4.3 9/28/16 Conditional Randomness Let \\(X_1, ..., X_n\\) denote a sequence of 0’s and 1’s. \\[ \\begin{aligned} P(X_1 = x_1, ..., X_n = x_n | X_1 + ... + X_n = T_n = m) &amp;= \\frac{\\prod_{i = 1}^n p^{x_i}(1-p)^{1-x_i}}{{n \\choose m} p^m(1-p)^{n-m}} \\\\ &amp;= \\frac{p^{\\sum_{i=1}^n x_i}(1-p)^{n - \\sum_{i=1}^n x_i}}{{n \\choose m} p^m(1-p)^{n-m}} \\\\ &amp;= \\frac{p^{m}(1-p)^{n - m}}{{n \\choose m} p^m(1-p)^{n-m}} \\\\ &amp;= \\frac{1}{{n \\choose m}} \\end{aligned} \\] Thus, given that the sum of \\(n\\) bernoullis is \\(m\\) then, each permutation of success and failures is equally likely. Let \\(k &lt; n\\), now given that \\(T_n = m\\), we have \\[ \\begin{aligned} P(T_k = i | T_n = m) &amp;= \\frac{P(T_k = i) P(T_{(k+1):n} = m-i)}{P(T_n = m)} \\\\ &amp;= \\frac{{k \\choose i}p^i(1-p)^{k-i}{n-k \\choose m-i}p^{m-i}(1-p)^{n-k-m+i}}{{n \\choose m}p^m(1-p)^{n-m}} \\\\ &amp;= \\frac{{k \\choose i}{n-k \\choose m-i}p^{m}(1-p)^{n-m}}{{n \\choose m}p^m(1-p)^{n-m}} \\\\ &amp;= \\frac{{k \\choose i}{n-k \\choose m-i}}{{n \\choose m}} \\end{aligned} \\] "],
["applications.html", "5 Applications 5.1 Example one 5.2 Example two", " 5 Applications Some significant applications are demonstrated in this chapter. 5.1 Example one 5.2 Example two "],
["final-words.html", "6 Final Words", " 6 Final Words We have finished a nice book. "],
["references.html", "References", " References "]
]
